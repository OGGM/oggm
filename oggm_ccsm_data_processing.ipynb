{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oggm \n",
    "from oggm import cfg, workflow, utils\n",
    "from oggm.workflow import execute_entity_task\n",
    "import geopandas as gpd\n",
    "import salem, os, netCDF4, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-28 16:53:07: oggm.cfg: Parameter file: /home/david/oggm/oggm/params.cfg\n"
     ]
    }
   ],
   "source": [
    "cfg.initialize()\n",
    "cfg.PARAMS['continue_on_error'] = True\n",
    "cfg.PATHS['climate_file'] = \"/home/david/Desktop/CCSM4_midHolocene_data/127ka.nc\"\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "PI_path = \"/home/david/Desktop/CCSM4_midHolocene_data/pi.nc\"\n",
    "rgi_version = '6'\n",
    "rgi_region = '17'\n",
    "cfg.PATHS['working_dir'] = \"/home/david/OGGM_WORKING_DIRECTORY/\"\n",
    "path = utils.get_rgi_intersects_region_file(rgi_region, version=rgi_version)\n",
    "cfg.set_intersects_db(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-28 16:36:44: oggm.workflow: Multiprocessing: using all available processors (N=12)\n"
     ]
    }
   ],
   "source": [
    "rgi = gpd.read_file(os.path.join(cfg.PATHS['working_dir'], 'rgi_pif.shp'))\n",
    "gdirs = oggm.workflow.init_glacier_regions(rgi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<oggm.GlacierDirectory>\n",
       "  RGI id: RGI60-17.00086\n",
       "  Region: 17: Southern Andes\n",
       "  Subregion: 17-01: Patagonia                       \n",
       "  Glacier type: Glacier\n",
       "  Terminus type: Land-terminating\n",
       "  Area: 0.093 km2\n",
       "  Lon, Lat: (-73.1545, -50.8267)\n",
       "  Grid (nx, ny): (85, 57)\n",
       "  Grid (dx, dy): (14.0, -14.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdir = gdirs[0]\n",
    "gdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.datetime(1950, 4, 1, 0, 0),\n",
       "       datetime.datetime(1950, 5, 1, 0, 0),\n",
       "       datetime.datetime(1950, 6, 1, 0, 0),\n",
       "       datetime.datetime(1950, 7, 1, 0, 0),\n",
       "       datetime.datetime(1950, 8, 1, 0, 0),\n",
       "       datetime.datetime(1950, 9, 1, 0, 0),\n",
       "       datetime.datetime(1950, 10, 1, 0, 0),\n",
       "       datetime.datetime(1950, 11, 1, 0, 0),\n",
       "       datetime.datetime(1950, 12, 1, 0, 0),\n",
       "       datetime.datetime(1951, 1, 1, 0, 0),\n",
       "       datetime.datetime(1951, 2, 1, 0, 0),\n",
       "       datetime.datetime(1951, 3, 1, 0, 0)], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = gdir.get_filepath('cesm_data')\n",
    "with netCDF4.Dataset(fpath, mode='r') as nc:\n",
    "    time = nc.variables['time']\n",
    "    time = netCDF4.num2date(time[:], time.units)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put this in the function def (see process_cesm_data)\n",
    "filesuffix=''\n",
    "\n",
    "if not (('climate_file' in cfg.PATHS) and\n",
    "        os.path.exists(cfg.PATHS['climate_file'])):\n",
    "    raise IOError('Custom climate file not found')\n",
    "\n",
    "#open dataset for precp use\n",
    "fpath = cfg.PATHS['climate_file']\n",
    "xr_ccsm = xr.open_dataset(fpath, decode_times=False)\n",
    "#open dataset for tmp use\n",
    "xr_ccsm_ts = xr.open_dataset(fpath)\n",
    "#repeating for pi\n",
    "xr_pi = xr.open_dataset(PI_path, decode_times=False)\n",
    "\n",
    "# selecting location\n",
    "lon = gdir.cenlon\n",
    "lat = gdir.cenlat\n",
    "\n",
    "#Setting the longitude to a 0-360 grid [I think...] \"CESM files are in 0-360\"\n",
    "if lon <= 0:\n",
    "    lon += 360\n",
    "\n",
    "#\"take the closest\"\n",
    "#\"TODO: consider GCM interpolation?\"\n",
    "precp = xr_ccsm.PRECC.sel(lat=lat, lon=lon, method='nearest') + xr_ccsm.PRECL.sel(lat=lat, lon=lon, method='nearest')\n",
    "temp = xr_ccsm_ts.TS.sel(lat=lat, lon=lon, method='nearest')\n",
    "precp_pi = xr_pi.PRECC.sel(lat=lat, lon=lon, method='nearest') + xr_pi.PRECL.sel(lat=lat, lon=lon, method='nearest')\n",
    "temp_pi = xr_pi.TS.sel(lat=lat, lon=lon, method='nearest')\n",
    "\n",
    "#convert temp from K to C\n",
    "temp = temp - 273.15\n",
    "temp_pi = temp_pi - 273.15\n",
    "\n",
    "#Take anomaly for C|CSM data (with preindustrial control)\n",
    "for i in range(12):\n",
    "    temp.values[i] = temp.values[i] - temp_pi.values[i]\n",
    "for i in range(12):\n",
    "    precp.values[i] = precp.values[i] - precp_pi.values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting & formatting values for y0 & y1\n",
    "y_0 = pd.to_datetime(str(temp.time.values[0]))\n",
    "y0= int(y_0.strftime('%Y'))\n",
    "y_1 = pd.to_datetime(str(temp.time.values[-1]))\n",
    "y1 = int(y_1.strftime('%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformatting the time element of the data array to a datetime64.\n",
    "time = pd.period_range('{}-02'.format(y0), '{}01'.format(y1), freq='M')\n",
    "temp['time'], precp['time'] = time, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for https://github.com/pydata/xarray/issues/1565\n",
    "temp['month'] = ('time', time.month)\n",
    "precp['month'] = ('time', time.month)\n",
    "temp['year'] = ('time', time.year)\n",
    "temp['year'] = ('time', time.year)\n",
    "ny, r = divmod(len(temp.time), 12)\n",
    "assert r == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert m s-1 to mm mth-1 (for precp)\n",
    "ndays = np.tile(cfg.DAYS_IN_MONTH, y1-y0)\n",
    "precp = precp * ndays * (60 * 60 * 24 * 1000)\n",
    "\n",
    "#\"Get CRU to apply the anomaly to\"\n",
    "fpath = gdir.get_filepath('climate_monthly')\n",
    "ds_cru = xr.open_dataset(fpath)\n",
    "\n",
    "#\"Here we assume the gradient is a monthly average\"\n",
    "ts_grad = np.tile(ds_cru.grad[0:12], ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"Add the climate anomaly to CRU clim\"\n",
    "dscru = ds_cru.sel(time=slice('1961', '1990'))\n",
    "#\"for temp\"\n",
    "loc_tmp = dscru.temp.groupby('time.month').mean()\n",
    "loc_tmp = xr.concat([loc_tmp[1:], loc_tmp[:-11]], dim='month')\n",
    "ts_tmp = temp #+ loc_tmp\n",
    "#ts_tmp[:] = [(i - 5) for i in temp]\n",
    "for j in range(12):\n",
    "    ts_tmp.values[j] = ts_tmp[j].values + loc_tmp.values[j]\n",
    "ts_tmp = xr.concat([ts_tmp[2:], ts_tmp[:-10]], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"for prcp\"\n",
    "loc_pre = dscru.prcp.groupby('time.month').mean()\n",
    "loc_pre = xr.concat([loc_pre[1:], loc_pre[:-11]], dim='month')\n",
    "ts_pre = precp\n",
    "for j in range(12):\n",
    "    ts_pre.values[j] = ts_pre[j].values + loc_pre.values[j]\n",
    "ts_pre = xr.concat([ts_pre[2:], ts_pre[:-10]], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = cfg.PATHS['climate_file']\n",
    "dsindex = salem.GeoNetcdf(fpath, monthbegin=True)\n",
    "time1 = dsindex.variables['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "time2 = [711475., 711505.,711536., 711566.,711597., 711628.,711658., 711689.,711719., 711750.,711781., 711809.]\n",
    "time2 = netCDF4.num2date(time2, time1.units, calendar='noleap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-04-28 16:37:45: __main__: RGI60-17.00086\n"
     ]
    }
   ],
   "source": [
    "assert np.all(np.isfinite(ts_pre.values))\n",
    "assert np.all(np.isfinite(ts_tmp.values))\n",
    "assert np.all(np.isfinite(ts_grad))\n",
    "\n",
    "#\"back to -180 - 180\"\n",
    "loc_lon = precp.lon if precp.lon <= 180 else precp.lon - 360\n",
    "\n",
    "#write to netcdf\n",
    "gdir.write_monthly_climate_file(time2, ts_pre.values, ts_tmp.values, ts_grad, float(dscru.ref_hgt), loc_lon, precp.lat.values, time_unit=time1.units, file_name='cesm_data', filesuffix=filesuffix)\n",
    "\n",
    "#dsindex._nc.close()\n",
    "xr_ccsm.close()\n",
    "xr_ccsm_ts.close()\n",
    "xr_pi.close()\n",
    "ds_cru.close()\n",
    "\n",
    "#logger message\n",
    "log.info(\"%s\", gdir.rgi_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCSM DATA PROCESSING -- > DO NOT GOOF WITH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading salem-sample-data...\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import salem, os, netCDF4, logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "def process_ccsm_data(gdir):\n",
    "    \n",
    "    \"\"\"\n",
    "        First attempt at a method to process the CCSM data into temperature/precip anomalies.\n",
    "\n",
    "    \"\"\"\n",
    " \n",
    "    #Put this in the function def (see process_cesm_data)\n",
    "    filesuffix=''\n",
    "\n",
    "    if not (('climate_file' in cfg.PATHS) and\n",
    "            os.path.exists(cfg.PATHS['climate_file'])):\n",
    "        raise IOError('Custom climate file not found')\n",
    "\n",
    "    #open dataset for precp use\n",
    "    fpath = cfg.PATHS['climate_file']\n",
    "    xr_ccsm = xr.open_dataset(fpath, decode_times=False)\n",
    "    #open dataset for tmp use\n",
    "    xr_ccsm_ts = xr.open_dataset(fpath, decode_times=False)\n",
    "    #repeating for pi\n",
    "    xr_pi = xr.open_dataset(PI_path, decode_times=False)\n",
    "\n",
    "    # selecting location\n",
    "    lon = gdir.cenlon\n",
    "    lat = gdir.cenlat\n",
    "\n",
    "    #Setting the longitude to a 0-360 grid [I think...] \"CESM files are in 0-360\"\n",
    "    if lon <= 0:\n",
    "        lon += 360 \n",
    "\n",
    "    #\"take the closest\"\n",
    "    #\"TODO: consider GCM interpolation?\"\n",
    "    precp = xr_ccsm.PRECC.sel(lat=lat, lon=lon, method='nearest') + xr_ccsm.PRECL.sel(lat=lat, lon=lon, method='nearest')\n",
    "    temp = xr_ccsm_ts.TS.sel(lat=lat, lon=lon, method='nearest')\n",
    "    precp_pi = xr_pi.PRECC.sel(lat=lat, lon=lon, method='nearest') + xr_pi.PRECL.sel(lat=lat, lon=lon, method='nearest')\n",
    "    temp_pi = xr_pi.TS.sel(lat=lat, lon=lon, method='nearest')\n",
    "\n",
    "    #convert temp from K to C\n",
    "    temp = temp - 273.15\n",
    "    temp_pi = temp_pi - 273.15\n",
    "\n",
    "   #Take anomaly for C|CSM data (with preindustrial control)\n",
    "    for i in range(12):\n",
    "        temp.values[i] = temp.values[i] - temp_pi.values[i]\n",
    "    for i in range(12):\n",
    "        precp.values[i] = precp.values[i] - precp_pi.values[i]\n",
    "\n",
    "    #selecting & formatting values for y0 & y1\n",
    "    y_0 = pd.to_datetime(str(temp.time.values[0]))\n",
    "    y0= int(y_0.strftime('%Y'))\n",
    "    y_1 = pd.to_datetime(str(temp.time.values[-1]))\n",
    "    y1 = int(y_1.strftime('%Y'))\n",
    "\n",
    "    #reformatting the time element of the data array to a datetime64.\n",
    "    time = pd.period_range('{}-02'.format(y0), '{}01'.format(y1), freq='M')\n",
    "    temp['time'], precp['time'] = time, time\n",
    "\n",
    "    temp['month'] = ('time', time.month)\n",
    "    precp['month'] = ('time', time.month)\n",
    "    temp['year'] = ('time', time.year)\n",
    "    temp['year'] = ('time', time.year)\n",
    "    ny, r = divmod(len(temp.time), 12)\n",
    "    assert r == 0\n",
    "\n",
    "    #Convert m s-1 to mm mth-1 (for precp)\n",
    "    ndays = np.tile(cfg.DAYS_IN_MONTH, y1-y0)\n",
    "    precp = precp * ndays * (60 * 60 * 24 * 1000)\n",
    "\n",
    "    #\"Get CRU to apply the anomaly to\"\n",
    "    fpath = gdir.get_filepath('climate_monthly')\n",
    "    ds_cru = xr.open_dataset(fpath)\n",
    "\n",
    "    #\"Here we assume the gradient is a monthly average\"\n",
    "    ts_grad = np.tile(ds_cru.grad[0:12], ny)\n",
    "\n",
    "    #\"Add the climate anomaly to CRU clim\"\n",
    "    dscru = ds_cru.sel(time=slice('1961', '1990'))\n",
    "    #\"for temp\"\n",
    "    loc_tmp = dscru.temp.groupby('time.month').mean()\n",
    "    loc_tmp = xr.concat([loc_tmp[1:], loc_tmp[:-11]], dim='month')\n",
    "    ts_tmp = temp #+ loc_tmp\n",
    "    for j in range(12):\n",
    "        ts_tmp.values[j] = ts_tmp[j].values + loc_tmp.values[j]\n",
    "    ts_tmp = xr.concat([ts_tmp[2:], ts_tmp[:-10]], dim='time')\n",
    "    #\"for prcp\"\n",
    "    loc_pre = dscru.prcp.groupby('time.month').mean()\n",
    "    loc_pre = xr.concat([loc_pre[1:], loc_pre[:-11]], dim='month')\n",
    "    ts_pre = precp\n",
    "    for j in range(12):\n",
    "        ts_pre.values[j] = ts_pre[j].values + loc_pre.values[j]\n",
    "    ts_pre = xr.concat([ts_pre[2:], ts_pre[:-10]], dim='time')\n",
    "\n",
    "    #\"load dates into right format to save\"\n",
    "    fpath = cfg.PATHS['climate_file']\n",
    "    dsindex = salem.GeoNetcdf(fpath, monthbegin=True)\n",
    "    time1 = dsindex.variables['time']\n",
    "\n",
    "    assert np.all(np.isfinite(ts_pre.values))\n",
    "    assert np.all(np.isfinite(ts_tmp.values))\n",
    "    assert np.all(np.isfinite(ts_grad))\n",
    "\n",
    "    time2 = [711475., 711505.,711536., 711566.,711597., 711628.,711658., 711689.,711719., 711750.,711781., 711809.]\n",
    "    time2 = netCDF4.num2date(time2, time1.units, calendar='noleap')\n",
    "\n",
    "    #\"back to -180 - 180\"\n",
    "    loc_lon = precp.lon if precp.lon <= 180 else precp.lon - 360\n",
    "\n",
    "    #write to netcdf\n",
    "    gdir.write_monthly_climate_file(time2, ts_pre.values, ts_tmp.values, ts_grad, float(dscru.ref_hgt), loc_lon, precp.lat.values, time_unit=time1.units, file_name='cesm_data', filesuffix=filesuffix)\n",
    "\n",
    "    #dsindex._nc.close()\n",
    "    xr_ccsm.close()\n",
    "    xr_ccsm_ts.close()\n",
    "    xr_pi.close()\n",
    "    ds_cru.close()\n",
    "\n",
    "    #logger message\n",
    "    log.info(\"%s\", gdir.rgi_id)\n",
    "    \n",
    "def process_ccsm_gdirs(gdirs):\n",
    "    for i in gdirs:\n",
    "        new_process_ccsm_data(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
